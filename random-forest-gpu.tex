\documentclass{lug}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{pseudocode}
\usepackage{romannum}
\setbeamertemplate {footline}{\Large{\quad\hfill\insertframenumber\strut\quad}}
\usepackage{algorithm2e}
\renewcommand{\thealgocf}{}

\title{Parallelism Insights into the Random Forest Algorithm}
\author{Lou Brand and Xun Li}
\institute{\textbf{Advanced High Performance Computing\\Colorado School of Mines}}
\date{\today}

\begin{document}

\section{Background}
\frame{
    \frametitle{Random Forest}
    \begin{center}
        \includegraphics[scale=0.6]{images/random-forest-simplified.jpg}\\
    \end{center}
}

\frame{
    \frametitle{Applications}
    \begin{center}
        \includegraphics[scale=0.75]{images/cifar1.jpg}\\ 
        Face perception, image tagging, regression, classification, etc.
    \end{center}
}

\frame{
    \frametitle{Towards an Alternative to Deep Neural Networks: gcForest}
    \begin{center}
        \includegraphics[scale=0.3]{images/gcForest.png}\\
        ``The efficiency of gcForest can be improved further with optimized parallel implementation'' - \textit{Zhou et.al.}
    \end{center}
}

\frame{
    \frametitle{An Insight into Node Splitting}
    \begin{itemize}[<+->]
        \item The elemental operation of the random forest algorithm
        \item More interesting than the embarassingly parallelism mentioned in gcForest
        \item Tree algorithms are inherently difficult to parallelize
            \begin{enumerate}
                \item Load balancing
                \item Dynamic irregular memory accesses
                \item As we further down in the tree we use many small kernels
            \end{enumerate}
    \end{itemize}
}

\section{Approach}
\frame{
    \frametitle{Node Splitting}
    \begin{center}
        \includegraphics[scale=0.5]{images/node-split.png}\\
        \textbf{Goal}: Optimize splitting of the nodes using parallel programming techniques 
    \end{center}
}

\frame{
    \frametitle{Maximize Information Gain}
    \begin{enumerate}[<+->]
        \item Choose a $feature$ to split on 
        \item Organize data samples into L and R children nodes 
        \item Calculate information gain: 
            $$ IG_{feature}=GI_P-\frac{N_L}{N}GI_L-\frac{N_R}{N}GI_R $$
        where,
            $$ GI=1- \sum_{j=0}^{M}(\frac{C_j}{W})^2 $$
        $W$ samples in node, $C_j$ number of samples w/ label $j$
        \item Repeat 1-3 for additional features
        \item Splitting feature $= argmax(IG_{feature})$
    \end{enumerate}
}

\section{Implementation and Experiments}

\frame{
    \frametitle{Experimental Setup}
    \textbf{Synthetic Data}
    \begin{enumerate}
        \item Dataset size: 8,388,608 samples 
        \item Feature range: 3,200 discrete values
        \item Labels: 32 classes
    \end{enumerate}
    \pause
    \textbf{Environment, AWS} Intel Xeon ES-2670
    \begin{enumerate}
        \item 8x hyper-threads (4 cores)
    \end{enumerate}
    \pause
    \textbf{Environment, AWS} NVIDIA GRID GPU (Kepler GK104)
    \begin{enumerate}
        \item Memory: 2 GB
        \item Memory bandwidth: 224.3 GB/sec
    \end{enumerate}
}

\frame{
    \frametitle{CPU Baseline}
    \begin{center}
        \Large{\textbf{Serial Runtime:} 94.97 secs\\ \textbf{OpenMP Runtime:} 16.77 secs}
    \end{center}
}

\setbeamercovered{invisible}
\frame{
    \frametitle{GPU Implementation--Atomic Block}
    \begin{center}
        \includegraphics[scale=.12]{images/parallel-histogram.jpg}\\
        \onslide<2->{\Large{\textbf{Runtime:} 43.42 sec\\ \textbf{Atomic Throughput:} 4.06 GB/sec$^*$}\\ \vspace{.5cm}\small{$^*$(on GT 750M) out of 80 GB/sec}}
    \end{center}
}

\frame{
    \frametitle{GPU Implementation--Atomic Interleaved}
    \begin{center}
        \includegraphics[scale=.13]{images/interleaved.jpg}\\
        \onslide<2->{\Large{\textbf{Runtime:} 8.57 sec\\ \textbf{Atomic Throughput:} 18.6 GB/sec$^*$}\\ \vspace{.5cm}\small{$^*$(on GT 750M) out of 80 GB/sec}}
    \end{center}
}

\frame{
    \frametitle{GPU Implementation--Atomic Privatization}
    \begin{center}
        \vspace{-.9cm}\includegraphics[scale=.45]{images/privitization-code.png}\\
        \onslide<2->{\Large{\textbf{Runtime:} 5.29 sec, \textbf{Atomic Thrpt:} 1.07 GB/sec$^*$}\\ \vspace{.3cm}\small{$^*$(on GT 750M) out of 80 GB/sec}}
    \end{center}
} 

\frame{
    \frametitle{GPU Implementation--Coarse-Grain Parallelism}
    \begin{pseudocode}{SplittingNode}{label,feature,S} 
        1\color{red}{\quad \FOR k \GETS 0 \TO max(S)}\\
    2\quad	 \quad \FOR i \GETS 0 \TO N \\
        3\quad \quad \text{Compute Histogram}\\
        4\quad \quad  N, N_L, N_R \GETS sum(b_P),sum(b_L),sum(b_R)\\
        5\quad \quad \text{Compute:} \quad GI_P, GI_L, GI_R\\
        6\quad \quad \text{Compute:} \quad IG[k]\\
    7\quad \quad split \GETS S[argmax(IG)]\\
    8\quad \RETURN{split, child}
    \end{pseudocode}
    \begin{center}
        \onslide<2->{\Large{\textbf{Runtime:} 5.23 sec}}
    \end{center}
}

\section{Insights}

\setbeamercovered{transparent}
\frame{
    \frametitle{What Should We Choose?}
    \textbf{Some Intuition}
    \begin{itemize}[<+->]
        \item GPU computation works well early on in the tree (when there are many instances to be bagged)
        \item Further down the tree the CPU implementation works best
    \end{itemize}
    \onslide<+->{\textbf{What to Choose}}
    \begin{itemize}[<+->]
    	\item CPU Serial: ``Smaller'' Datasets
        \item CPU Parallel (OpenMP): ``Small'' Datasets
        \item GPU Privatization: ``High Contention'' Datasets
        \item GPU Coarse-Grain: ``Large Number of Features'' Datasets
    \end{itemize}
}

\frame{
    \frametitle{Future Work}
    \textbf{Algorithmic Changes to Improve Redundancy}
    \begin{enumerate}[<+->]
        \item The first node split can be precomputed and stored
        \item Pooling some tasks via node merge, helps with load balance issues 
        \item Pipe-lining with CPU and GPU
    \end{enumerate}
    \onslide<+->{\textbf{Parameter Tuning for Further Insight}}
    \begin{enumerate}[<+->]
        \item Sample size
        \item Threads in block
    \end{enumerate}
}

\frame{
    \frametitle{Conclusions}
    \textbf{What Did We Learn?}
    \begin{itemize}[<+->]
        \item Six implementations of node splitting (CPU/GPU)
        \item Deeper understanding of the performance bottlenecks
        \item Insights into the random forest optimization at a fine and coarse-grained level 
    \end{itemize}
}

\frame{
    \begin{center}
        \Huge{Questions/Discussion}\\
    \end{center}
}

\nocite{*} % Include everything in the .bib file.
\bibliographystyle{plainnat}
\bibliography{random-forest-gpu}

% that's all, folks
\end{document}
